# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M1dglGJEMjQe5_1ToYAgOyRkVpwz_2RA
"""

!pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai

import requests
from bs4 import BeautifulSoup

# Function to scrape website content
def scrape_uchicago_data(url="https://www.uchicago.edu/"):
    # Fetch the webpage
    response = requests.get(url)

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Extract all paragraph texts
    paragraphs = soup.find_all('p')
    content = [para.get_text() for para in paragraphs]

    return content  # Return extracted content

# Scrape content from University of Chicago
uchicago_content = scrape_uchicago_data()
print(uchicago_content[:5])  # Print first 5 paragraphs to verify

from sentence_transformers import SentenceTransformer

# Function to generate embeddings
def generate_embeddings(text_data):
    # Load the pre-trained Sentence-BERT model
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Convert text data (list of paragraphs) into embeddings
    embeddings = model.encode(text_data)

    return embeddings

# Generate embeddings for the University of Chicago content
embeddings = generate_embeddings(uchicago_content)
print(embeddings[:5])  # Print first 5 embeddings to verify

import faiss
import numpy as np

# Function to store embeddings in FAISS
def store_embeddings_in_faiss(embeddings):
    # Convert embeddings list into a numpy array (required by FAISS)
    embedding_array = np.array(embeddings).astype('float32')

    # Create a FAISS index (this is where we store the embeddings)
    index = faiss.IndexFlatL2(embedding_array.shape[1])  # L2 distance for similarity search
    index.add(embedding_array)  # Add embeddings to the index

    return index

# Store the embeddings in FAISS
faiss_index = store_embeddings_in_faiss(embeddings)
print(f"FAISS Index: {faiss_index}")

def search_embeddings(query, model, index, top_k=5):
    # Convert the query into an embedding
    query_embedding = model.encode([query])  # We wrap it in a list for consistency

    # Search for the top-k most similar embeddings
    distances, indices = index.search(np.array(query_embedding).astype('float32'), top_k)

    return distances, indices

# Test the search function with a sample query
user_query = "What programs does the University of Chicago offer?"
distances, indices = search_embeddings(user_query, SentenceTransformer('all-MiniLM-L6-v2'), faiss_index)

# Get the top-k most similar content
retrieved_chunks = [uchicago_content[i] for i in indices[0]]
print("Retrieved Chunks:", retrieved_chunks)

# Upgrade the OpenAI Python package to the latest version
!pip install --upgrade openai

!pip install transformers

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the GPT-2 model and tokenizer from Hugging Face
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

def generate_response_gpt2(retrieved_chunks, query):
    # Combine retrieved chunks and query into a single context
    context = "\n".join(retrieved_chunks) + "\n" + query

    # Tokenize the input (context + query) and convert it to tensor format
    inputs = tokenizer.encode(context, return_tensors="pt")

    # Generate a response from GPT-2
    outputs = model.generate(
        inputs,
        max_length=200,  # You can adjust this value based on the desired length
        num_return_sequences=1,  # Number of responses to generate
        no_repeat_ngram_size=2,  # To avoid repetitive text generation
        temperature=0.7,  # Controls the randomness (lower = more focused)
    )

    # Decode the generated response back to text
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip()

# Sample retrieved chunks (representing relevant information retrieved from a database)
retrieved_chunks = [
    "The University of Chicago offers undergraduate and graduate programs in various fields, including arts, sciences, engineering, and business.",
    "It is known for its rigorous academic programs, including top-tier research opportunities and a strong focus on interdisciplinary studies."
]

# Sample user query
user_query = "What programs does the University of Chicago offer?"

# Generate response
response = generate_response_gpt2(retrieved_chunks, user_query)
print("Generated Response:", response)

import faiss
import numpy as np

# Function to store embeddings in FAISS
def store_embeddings_in_faiss(embeddings):
    # Convert embeddings list into a numpy array (required by FAISS)
    embeddings_np = np.array(embeddings).astype('float32')

    # Create a FAISS index (for L2 distance)
    index = faiss.IndexFlatL2(embeddings_np.shape[1])  # embeddings_np.shape[1] is the dimension of the embeddings

    # Add the embeddings to the FAISS index
    index.add(embeddings_np)

    return index

# Store embeddings in FAISS
faiss_index = store_embeddings_in_faiss(embeddings)
print("FAISS Index built and embeddings stored.")

# Function to search for the nearest neighbor
def search_similar_paragraphs(query, index, embeddings, top_k=5):
    # Generate embedding for the query using the same model
    query_embedding = generate_embeddings([query])[0].reshape(1, -1).astype('float32')

    # Perform the search in the FAISS index
    distances, indices = index.search(query_embedding, top_k)

    # Get the top_k most similar paragraphs
    similar_paragraphs = [uchicago_content[i] for i in indices[0]]
    return similar_paragraphs, distances[0]

# Example query
query = "What is the mission of the university?"
similar_paragraphs, distances = search_similar_paragraphs(query, faiss_index, embeddings)

# Print the results
print("Top 5 most similar paragraphs:")
for para, dist in zip(similar_paragraphs, distances):
    print(f"Distance: {dist:.4f}, Paragraph: {para[:150]}...")  # Print first 150 characters

faiss.write_index(faiss_index, "uchicago_index.index")

pip install Flask

from flask import Flask, request, render_template
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

app = Flask(__name__)

# Load the FAISS index and the embedding model
faiss_index = faiss.read_index("uchicago_index.index")
model = SentenceTransformer('all-MiniLM-L6-v2')

# Load the content (you should have this from your previous scraping)
uchicago_content = [...]  # Replace with your actual content list

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/query', methods=['POST'])
def query():
    user_query = request.form['query']
    similar_paragraphs, distances = search_similar_paragraphs(user_query, faiss_index)
    response = generate_response_gpt2(similar_paragraphs, user_query)
    return render_template('index.html', query=user_query, response=response)

def search_similar_paragraphs(query, index, top_k=5):
    query_embedding = model.encode([query])[0].reshape(1, -1).astype('float32')
    distances, indices = index.search(query_embedding, top_k)
    similar_paragraphs = [uchicago_content[i] for i in indices[0]]
    return similar_paragraphs, distances[0]

def generate_response_gpt2(retrieved_chunks, query):
    context = "\n".join(retrieved_chunks) + "\n" + query
    inputs = tokenizer.encode(context, return_tensors="pt")
    outputs = model.generate(inputs, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip()

if __name__ == '__main__':
    app.run(debug=True)